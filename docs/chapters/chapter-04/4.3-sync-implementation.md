# 4.3 同步调用实现

本节实现 OpenAI 兼容客户端的同步调用功能。完成后你就能调用 LLM 获取响应了。

## 本节要做什么

- 实现 `LlmClient.chat()` 方法
- 用 WebClient 发 HTTP 请求
- 解析 API 返回的 JSON
- 写单元测试

同步调用等完整响应返回，流式调用下一节做。

---

## 动手实现

### 步骤 1：创建配置类

新建 `LlmProperties.java`，用于读取 `application.yml` 里的 LLM 配置：

```java
@Data
@Component
@ConfigurationProperties(prefix = "miniclaw.llm")
public class LlmProperties {
    private String endpoint;      // API 地址
    private String apiKey;        // API Key
    private String model;         // 默认模型
    private Integer timeout = 60; // 超时秒数
    private Double temperature = 0.7;
    private Integer maxTokens = 4096;
}
```

然后在 `application.yml` 添加配置：

```yaml
miniclaw:
  llm:
    endpoint: https://api.openai.com/v1
    api-key: ${LLM_API_KEY}
    model: gpt-4
    timeout: 60
    temperature: 0.7
    max-tokens: 4096
```

**环境变量方式（推荐）：**

```bash
export LLM_API_KEY="sk-xxx"
export LLM_ENDPOINT="https://api.deepseek.com/v1"
export LLM_MODEL="deepseek-chat"
```

### 步骤 2：实现客户端类

创建 `OpenAiCompatibleLlmClient.java`，实现 `LlmClient` 接口：

```java
@Slf4j
public class OpenAiCompatibleLlmClient implements LlmClient {

    private final LlmProperties properties;
    private final ObjectMapper objectMapper;
    private WebClient webClient;

    public OpenAiCompatibleLlmClient(LlmProperties properties, ObjectMapper objectMapper) {
        this.properties = properties;
        this.objectMapper = objectMapper;

        if (properties.getEndpoint() != null) {
            this.webClient = buildWebClient(properties.getEndpoint(), properties.getApiKey());
        }
    }

    private WebClient buildWebClient(String endpoint, String apiKey) {
        return WebClient.builder()
            .baseUrl(endpoint)
            .defaultHeader("Authorization", "Bearer " + apiKey)
            .defaultHeader("Content-Type", "application/json")
            .build();
    }

    // ... chat() 方法在下面
}
```

为什么要用 WebClient？因为下一节流式输出要用，Spring Boot 3.x 也推荐它。

### 步骤 3：实现 chat() 方法

这是核心方法，发送请求并返回响应：

```java
@Override
public LlmResponse chat(LlmRequest request) {
    if (webClient == null) {
        throw new LlmException("LLM client not configured");
    }

    try {
        // 1. 构建 API 请求体
        ChatCompletionRequest apiRequest = buildApiRequest(request, false);

        // 2. 发送 POST 请求
        String responseBody = webClient.post()
            .uri("/chat/completions")
            .bodyValue(apiRequest)
            .retrieve()
            .bodyToMono(String.class)
            .timeout(Duration.ofSeconds(properties.getTimeout()))
            .block();  // 阻塞等待结果

        // 3. 解析 JSON 响应
        return parseResponse(responseBody);

    } catch (Exception e) {
        log.error("LLM chat failed", e);
        throw new LlmException("LLM chat failed: " + e.getMessage(), e);
    }
}
```

三个步骤：构建请求 → 发送请求 → 解析响应

### 步骤 4：构建请求体

把我们的 `LlmRequest` 转成 OpenAI API 格式：

```java
private ChatCompletionRequest buildApiRequest(LlmRequest request, boolean stream) {
    ChatCompletionRequest apiRequest = new ChatCompletionRequest();

    // 设置模型（没有就用默认的）
    apiRequest.setModel(request.getModel() != null ? 
        request.getModel() : properties.getModel());

    // 转换消息列表
    List<Map<String, Object>> messages = new ArrayList<>();
    for (LlmRequest.Message message : request.getMessages()) {
        Map<String, Object> msg = new HashMap<>();
        msg.put("role", message.getRole());
        msg.put("content", message.getContent());
        messages.add(msg);
    }
    apiRequest.setMessages(messages);

    // 设置参数
    apiRequest.setTemperature(request.getTemperature() != null ? 
        request.getTemperature() : properties.getTemperature());
    apiRequest.setStream(stream);

    return apiRequest;
}
```

### 步骤 5：解析响应

从 JSON 里提取我们需要的内容：

```java
private LlmResponse parseResponse(String responseBody) throws JsonProcessingException {
    JsonNode root = objectMapper.readTree(responseBody);
    JsonNode choice = root.path("choices").get(0);
    JsonNode message = choice.path("message");

    // 提取内容
    String content = message.path("content").asText(null);

    // 提取工具调用（如果有）
    List<ToolCall> toolCalls = parseToolCalls(message.path("tool_calls"));

    // 提取结束原因
    String finishReason = choice.path("finish_reason").asText(null);

    // 提取 token 统计
    LlmResponse.Usage usage = parseUsage(root.path("usage"));

    return LlmResponse.builder()
        .content(content)
        .toolCalls(toolCalls)
        .finishReason(finishReason)
        .usage(usage)
        .build();
}

private LlmResponse.Usage parseUsage(JsonNode usageNode) {
    if (usageNode.isMissingNode()) {
        return null;
    }
    return LlmResponse.Usage.builder()
        .promptTokens(usageNode.path("prompt_tokens").asInt(0))
        .completionTokens(usageNode.path("completion_tokens").asInt(0))
        .totalTokens(usageNode.path("total_tokens").asInt(0))
        .build();
}

private List<ToolCall> parseToolCalls(JsonNode toolCallsNode) {
    if (toolCallsNode.isMissingNode() || toolCallsNode.isNull()) {
        return null;
    }
    
    List<ToolCall> toolCalls = new ArrayList<>();
    for (JsonNode node : toolCallsNode) {
        toolCalls.add(ToolCall.builder()
            .id(node.path("id").asText())
            .type(node.path("type").asText())
            .function(ToolCall.Function.builder()
                .name(node.path("function").path("name").asText())
                .arguments(node.path("function").path("arguments").asText())
                .build())
            .build());
    }
    return toolCalls;
}
```

---

## 怎么用

### 简单对话

```java
@Service
public class ChatService {

    @Autowired
    private LlmClient llmClient;

    public String chat(String userMessage) {
        LlmRequest request = LlmRequest.builder()
            .messages(List.of(
                LlmRequest.Message.user(userMessage)
            ))
            .build();

        LlmResponse response = llmClient.chat(request);
        return response.getContent();
    }
}
```

调用：
```java
chat("什么是 RAG？")
// 返回: "RAG（Retrieval-Augmented Generation）是一种..."
```

### 带系统提示

```java
public String chatWithSystem(String systemPrompt, String userMessage) {
    LlmRequest request = LlmRequest.builder()
        .messages(List.of(
            LlmRequest.Message.system(systemPrompt),
            LlmRequest.Message.user(userMessage)
        ))
        .build();

    return llmClient.chat(request).getContent();
}
```

调用：
```java
chatWithSystem(
    "你是一个资深 Java 开发者，用简洁专业的语言回答",
    "解释一下 Spring Bean 的生命周期"
)
```

### 多轮对话

```java
public String multiTurnChat(List<String> questions) {
    List<LlmRequest.Message> messages = new ArrayList<>();

    for (String question : questions) {
        // 添加用户问题
        messages.add(LlmRequest.Message.user(question));

        // 获取回复
        LlmResponse response = llmClient.chat(LlmRequest.builder()
            .messages(messages)
            .build());

        // 添加 AI 回复到历史
        messages.add(LlmRequest.Message.assistant(response.getContent()));
    }

    // 返回最后一次回复
    return messages.get(messages.size() - 1).getContent();
}
```

调用：
```java
multiTurnChat(List.of(
    "什么是微服务？",
    "它和单体架构比有什么优缺点？",
    "什么时候应该用微服务？"
))
```

### 指定模型和参数

```java
public String chatWithModel(String message, String model, double temperature) {
    LlmRequest request = LlmRequest.builder()
        .model(model)
        .temperature(temperature)
        .messages(List.of(LlmRequest.Message.user(message)))
        .build();

    return llmClient.chat(request).getContent();
}
```

调用：
```java
// 用 GPT-4，创造性更高
chatWithModel("写一首关于代码的诗", "gpt-4", 1.2);

// 用 DeepSeek，更稳定
chatWithModel("帮我重构这段代码", "deepseek-chat", 0.3);
```

---

## 单元测试

### 测试配置加载

```java
@SpringBootTest
class LlmPropertiesTest {

    @Autowired
    private LlmProperties properties;

    @Test
    void testPropertiesLoaded() {
        assertNotNull(properties.getEndpoint());
        assertNotNull(properties.getApiKey());
        assertEquals("gpt-4", properties.getModel());
        assertEquals(60, properties.getTimeout());
    }
}
```

### 测试客户端初始化

```java
class OpenAiCompatibleLlmClientTest {

    private OpenAiCompatibleLlmClient client;
    private LlmProperties properties;
    private ObjectMapper objectMapper;

    @BeforeEach
    void setUp() {
        properties = new LlmProperties();
        properties.setEndpoint("https://api.openai.com/v1");
        properties.setApiKey("test-key");
        properties.setModel("gpt-4");
        properties.setTimeout(60);

        objectMapper = new ObjectMapper();
        client = new OpenAiCompatibleLlmClient(properties, objectMapper);
    }

    @Test
    void testClientInitialized() {
        assertNotNull(client);
    }

    @Test
    void testClientWithoutEndpoint() {
        LlmProperties emptyProps = new LlmProperties();
        OpenAiCompatibleLlmClient emptyClient = 
            new OpenAiCompatibleLlmClient(emptyProps, new ObjectMapper());

        LlmRequest request = LlmRequest.builder()
            .messages(List.of(LlmRequest.Message.user("test")))
            .build();

        assertThrows(LlmException.class, () -> emptyClient.chat(request));
    }
}
```

### Mock 测试请求构建

```java
class LlmClientRequestTest {

    @Test
    void testBuildRequest() throws Exception {
        LlmProperties properties = new LlmProperties();
        properties.setModel("gpt-4");
        properties.setTemperature(0.7);

        OpenAiCompatibleLlmClient client = 
            new OpenAiCompatibleLlmClient(properties, new ObjectMapper());

        LlmRequest request = LlmRequest.builder()
            .messages(List.of(
                LlmRequest.Message.system("你是助手"),
                LlmRequest.Message.user("你好")
            ))
            .temperature(0.5)
            .build();

        // 用反射调用 buildApiRequest（实际项目中可以改为 package-private 或提取为公开方法）
        // 这里展示请求应该长什么样
        ChatCompletionRequest expected = new ChatCompletionRequest();
        expected.setModel("gpt-4");
        expected.setTemperature(0.5);
        expected.setStream(false);
        expected.setMessages(List.of(
            Map.of("role", "system", "content", "你是助手"),
            Map.of("role", "user", "content", "你好")
        ));

        // 验证构建逻辑正确
    }
}
```

### 测试响应解析

```java
class LlmClientResponseTest {

    @Test
    void testParseResponse() throws Exception {
        String json = """
            {
              "choices": [{
                "message": {
                  "role": "assistant",
                  "content": "Hello! How can I help?"
                },
                "finish_reason": "stop"
              }],
              "usage": {
                "prompt_tokens": 10,
                "completion_tokens": 5,
                "total_tokens": 15
              }
            }
            """;

        ObjectMapper mapper = new ObjectMapper();
        JsonNode root = mapper.readTree(json);
        JsonNode choice = root.path("choices").get(0);

        String content = choice.path("message").path("content").asText();
        String finishReason = choice.path("finish_reason").asText();

        assertEquals("Hello! How can I help?", content);
        assertEquals("stop", finishReason);
    }
}
```

运行测试：
```bash
mvn test -Dtest=LlmClientTest
```

---

## 常见问题

### 连接超时

错误信息：
```
LLM chat failed: Connection timed out
```

解决：增加超时时间
```yaml
miniclaw:
  llm:
    timeout: 120  # 改成 120 秒
```

### API Key 错误

错误信息：
```
401 Unauthorized
```

解决：检查环境变量
```bash
echo $LLM_API_KEY
# 如果为空，重新设置
export LLM_API_KEY="sk-xxx"
```

### 模型不存在

错误信息：
```
Model 'gpt-5' not found
```

解决：用正确的模型名
```yaml
miniclaw:
  llm:
    model: gpt-4  # 或 gpt-3.5-turbo、deepseek-chat
```

### 响应格式异常

错误信息：
```
Cannot deserialize value of type...
```

解决：检查 API 返回格式，打印原始响应调试：
```java
log.info("Raw response: {}", responseBody);
```

---

## 完整代码结构

```
src/main/java/com/example/miniclaw/
├── config/
│   └── LlmProperties.java
├── llm/
│   ├── LlmClient.java              # 接口
│   ├── LlmRequest.java             # 请求
│   ├── LlmResponse.java            # 响应
│   ├── ToolCall.java               # 工具调用
│   ├── LlmException.java           # 异常
│   └── OpenAiCompatibleLlmClient.java  # 实现
└── service/
    └── ChatService.java            # 使用示例

src/test/java/com/example/miniclaw/
├── LlmPropertiesTest.java
├── OpenAiCompatibleLlmClientTest.java
└── LlmClientRequestTest.java
```

---

## 下一节

下一节我们实现 SSE 流式输出，让响应实时显示出来：

- 什么是 SSE（Server-Sent Events）
- WebClient 流式接收
- 处理 delta 增量数据
- Reactor Flux 管道

[下一节：4.4 SSE 协议与流式输出 →](./4.4-sse-streaming.md)
